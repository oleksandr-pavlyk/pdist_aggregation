{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a5d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import show_versions\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"benchmarks/results/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "commit = str(subprocess.check_output(['git', 'rev-parse', 'HEAD'])).replace(\"b'\", \"\").replace(\"\\\\n'\", \"\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b2e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"n_samples_train\", \"n_samples_test\", \"n_features\", \"n_neighbors\"]\n",
    "df[cols] = df[cols].astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby([\"n_samples_train\", \"n_samples_test\", \"n_features\", \"n_neighbors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe52708",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, (vals, df) in enumerate(df_grouped):\n",
    "    # 16:9 ratio\n",
    "    fig = plt.figure(figsize=(24, 13.5))\n",
    "    ax = plt.gca()\n",
    "    splot = sns.barplot(y=\"chunk_info\", x=\"throughput\", hue=\"implementation\", data=df, ax=ax)\n",
    "    _ = ax.set_xlabel(\"Thoughput (in GB/s)\")\n",
    "    _ = ax.set_ylabel(\"Chunk size (number of vectors), Number of X_train chunks\")\n",
    "    _ = ax.tick_params(labelrotation=45)\n",
    "    for p in splot.patches:\n",
    "        _ = splot.annotate(f\"{p.get_width():.4e}\", \n",
    "                       (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                       ha = 'center', va = 'center', \n",
    "                       size=10,\n",
    "                       xytext = (0, -12), \n",
    "                       textcoords = 'offset points')\n",
    "    title = f\"NearestNeighbors@{commit} - Euclidean Distance, dtype=np.float64, {df.trial.max() + 1} trials\\n\"\n",
    "    title += \", \".join([f\"{var}={os.environ.get(var)}\" for var in [\"OMP_NUM_THREADS\", \"OPENBLAS_NUM_THREADS\", \"MKL_NUM_THREADS\"]]) + \"\\n\"\n",
    "    title += \"n_samples_train=%s - n_samples_test=%s - n_features=%s - n_neighbors=%s\" % vals\n",
    "    _ = fig.suptitle(title, fontsize=16)\n",
    "    plt.savefig(f\"{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-vessel",
   "metadata": {},
   "source": [
    "### Machine specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /proc/version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "! lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcc -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "! env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-romania",
   "metadata": {},
   "source": [
    "### Environment specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-drama",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-pitch",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "! date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c1a0f5",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16bd97",
   "metadata": {},
   "source": [
    "### Cache analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a850f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = str(subprocess.check_output(['getconf','-a'])).replace(\"b'\", \"\").split(\"\\\\n\")\n",
    "cache_info = {}\n",
    "for line in out:\n",
    "    info = line.split(\" \")\n",
    "    if 'cache' in info[0].lower():\n",
    "        cache_info[info[0]] = int(info[-1]) if info[-1] != '' else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c583f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962de1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def what_fits(\n",
    "    n: int,\n",
    "    d: int,\n",
    "    k: int,\n",
    "    cache_info: dict,\n",
    "    fdtype=np.float64,\n",
    "    idtype=np.int64,\n",
    "    verbose=False,\n",
    "    parallel_on_xtrain=True,\n",
    "):\n",
    "    \"\"\" Quick analysis to see what fits in the caches.\n",
    "    \n",
    "    @param n: chunk_size\n",
    "    @param d: number of features\n",
    "    @param d: number of neighbours\n",
    "    @param cache_info: machine caches' information\n",
    "    @verbose cache_info: machine caches' information\n",
    "    @parallel_on_xtrain: if True, analyse for the\n",
    "        implementation parallelising on X_train, else\n",
    "        analyse for the one on X_test\n",
    "    \"\"\"\n",
    "    # dtype size\n",
    "    sf = 8 if fdtype == np.float64 else 4\n",
    "    si = 8 if idtype == np.int64 else 4\n",
    "    \n",
    "    # dist_middle_terms_chunks\n",
    "    p_distance_matrix_size = n * n * sf\n",
    "    \n",
    "    # X_train_sq_norms\n",
    "    x_train_sq_norms = n * sf\n",
    "    \n",
    "    # X_train[X_train_start:X_train_end, :] and\n",
    "    # X_test[X_test_start:X_test_end, :]\n",
    "    chunk_vects_size = 2 * n * d * sf\n",
    "    \n",
    "    # knn_indices and knn_red_distances\n",
    "    chunk_heaps_size = n * k * (sf + si)\n",
    "    \n",
    "    # heaps_indices_chunks, needed for synchronisation\n",
    "    extra_chunk_heaps_size = n * k * si if parallel_on_xtrain else 0\n",
    "\n",
    "    total = (\n",
    "        p_distance_matrix_size + \n",
    "        x_train_sq_norms + \n",
    "        chunk_vects_size + \n",
    "        chunk_heaps_size + \n",
    "        extra_chunk_heaps_size\n",
    "    )\n",
    "    \n",
    "    print(f\"Datastructure sizes for (n, d, k) = ({n}, {d}, {k}): {total} bytes\")\n",
    "    for level in ['LEVEL1_I', 'LEVEL1_D', 'LEVEL2_', 'LEVEL3_']:\n",
    "        cache_size = cache_info[f'{level}CACHE_SIZE']\n",
    "        if total < cache_size:\n",
    "            print(f\"    fits in {level} of size: {cache_size} bytes\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Datastructure sizes for (n, d, k) = ({n}, {d}, {k}):\")\n",
    "        print(f\"Pairwise distance matrix :   {p_distance_matrix_size}\")\n",
    "        print(f\"X_train squared norms    :   {x_train_sq_norms}\")\n",
    "        print(f\"X and Y chunks vectors   :   {chunk_vects_size}\")\n",
    "        print(f\"Heap on chunks           :   {chunk_heaps_size}\")\n",
    "        print(f\"Extra heap for sync.     :   {extra_chunk_heaps_size}\")\n",
    "        print(\"-------------------------------------\")\n",
    "        print(f\"Total                    :   {total}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9fa476",
   "metadata": {},
   "outputs": [],
   "source": [
    "what_fits(128, 50, 100, cache_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5362689",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = df[['chunk_info', 'n_features', 'n_neighbors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f205b43d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comb['chunk_size'] = comb['chunk_info'].apply(lambda s: int(s.split(',')[0].split('(')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = comb[['chunk_size', 'n_features', 'n_neighbors']].query('chunk_size > 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f45885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in combinations.itertuples():\n",
    "    what_fits(c.chunk_size, c.n_neighbors, c.n_neighbors, cache_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494e15c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
